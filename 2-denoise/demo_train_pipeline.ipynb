{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCAD-RT training pipeline            \n",
    "<img src=\"https://github.com/STAR-811/DeepCAD-RT-old/blob/main/images/logo-new.png?raw=true\" width = \"650\" height = \"180\" align=right />\n",
    "\n",
    "This file will demonstrate the basic pipeline for training DeepCAD-RT. A TIFF file will be downloaded automatically to be the example data. More information about the method and relevant results can be found in the companion paperï¼š\n",
    "\n",
    "**Real-time denoising of fluorescence time-lapse imaging enables high-sensitivity observations of biological dynamics beyond the shot-noise limit. bioRxiv (2022).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepcad.train_collection import training_class\n",
    "from deepcad.movie_display import display\n",
    "from deepcad.utils import get_first_filename,download_demo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select file(s) to be processed (download if not exist)\n",
    "The `download_demo` function will download a demo file and return the full path of it. This demo file will be stored in `/datasets`. If you want to use your own data for training, please create a new folder in `/datasets` and copy your data into it. \n",
    "Then, just change `datasets_path` into the name of your dataset folder. All TIFF files inside the dataset folder will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path(\"../dataset/dati_per_erzelli/mini2p_astro/good_example/2024Feb06-007\")\n",
    "sample_dir = Path(\"../dataset/sample\")\n",
    "dataset = dataset / \"motion_corrected\"\n",
    "dataset_small = sample_dir / \"motion_corrected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters for training\n",
    "Default setting shows the parameters for the demo file, which are also appropriate for most data. You can change these parameters according to your data and device. To visualize the training process, you can set the flags `visualize_images_per_epoch` and `save_test_images_per_epoch` according to your demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5                # number of training epochs\n",
    "GPU = '0'                   # the index of GPU you will use (e.g. '0', '0,1', '0,1,2')\n",
    "train_datasets_size = 1000  # datasets size for training (how many 3D patches) 3000\n",
    "patch_xy = 150              # the width and height of 3D patches\n",
    "patch_t = 150               # the time dimension (frames) of 3D patches\n",
    "overlap_factor = 0.4        # the overlap factor between two adjacent patches\n",
    "pth_dir = './pth'           # the path for pth file and result images \n",
    "num_workers = 0             # if you use Windows system, set this to 0.\n",
    "\n",
    "# Setup some parameters for result visualization during training period (optional)\n",
    "visualize_images_per_epoch = True  # whether to show result images after each epoch\n",
    "save_test_images_per_epoch = True  # whether to save result images after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Show the input low-SNR data  (optional)\n",
    "Play an input video (optional). This will load the video into memory and it is not an indispensable step. OpenCV library was used for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mDisplaying the first raw file -----> \u001b[0m\n",
      "../dataset/sample/motion_corrected/oabf_good_example.tiff\n"
     ]
    }
   ],
   "source": [
    "display_images = True\n",
    "\n",
    "if display_images:\n",
    "    display_filename = get_first_filename(str(dataset_small))\n",
    "    print('\\033[1;31mDisplaying the first raw file -----> \\033[0m')\n",
    "    print(display_filename)\n",
    "    display_length = 300     # how many frames to display\n",
    "    # normalize the image and display\n",
    "    display(display_filename, display_length=display_length, norm_min_percent=1, norm_max_percent=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training object\n",
    "This will creat a training object by passing all parameters as a dictionary. Parameters not specified in the dictionary will use their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mTraining parameters -----> \u001b[0m\n",
      "{'overlap_factor': 0.4, 'datasets_path': '../dataset/dati_per_erzelli/mini2p_astro/good_example/2024Feb06-007/motion_corrected', 'n_epochs': 5, 'fmap': 16, 'output_dir': './results', 'pth_dir': './pth', 'onnx_dir': './onnx', 'batch_size': 1, 'patch_t': 150, 'patch_x': 150, 'patch_y': 150, 'gap_y': 90, 'gap_x': 90, 'gap_t': 90, 'lr': 5e-05, 'b1': 0.5, 'b2': 0.999, 'GPU': '0', 'ngpu': 1, 'num_workers': 0, 'scale_factor': 1, 'train_datasets_size': 1000, 'select_img_num': 1000000, 'test_datasize': 400, 'visualize_images_per_epoch': True, 'save_test_images_per_epoch': True, 'colab_display': False, 'result_display': ''}\n"
     ]
    }
   ],
   "source": [
    "train_dict = {\n",
    "    # dataset dependent parameters\n",
    "    'patch_x': patch_xy,                          # the width of 3D patches\n",
    "    'patch_y': patch_xy,                          # the height of 3D patches\n",
    "    'patch_t': patch_t,                           # the time dimension (frames) of 3D patches\n",
    "    'overlap_factor':overlap_factor,              # overlap factor\n",
    "    'scale_factor': 1,                            # the factor for image intensity scaling\n",
    "    'select_img_num': 1000000,                    # select the number of frames used for training (use all frames by default)\n",
    "    'train_datasets_size': train_datasets_size,   # datasets size for training (how many 3D patches)\n",
    "    'datasets_path': str(dataset),               # folder containing files for training\n",
    "    'pth_dir': pth_dir,                           # the path for pth file and result images \n",
    "    \n",
    "    # network related parameters\n",
    "    'n_epochs': n_epochs,                         # the number of training epochs\n",
    "    'lr': 0.00005,                                # learning rate\n",
    "    'b1': 0.5,                                    # Adam: bata1\n",
    "    'b2': 0.999,                                  # Adam: bata2\n",
    "    'fmap': 16,                                   # model complexity\n",
    "    'GPU': GPU,                                   # GPU index\n",
    "    'num_workers': num_workers,                   # if you use Windows system, set this to 0.\n",
    "    'visualize_images_per_epoch': visualize_images_per_epoch,   # whether to show result images after each epoch\n",
    "    'save_test_images_per_epoch': save_test_images_per_epoch    # whether to save result images after each epoch\n",
    "}\n",
    "\n",
    "tc = training_class(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training process\n",
    "\n",
    "Here we lanuch the training process. The model of each epoch will be saved in the `/pth` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
